@article{Benedetti2019,
abstract = {Hybrid quantum-classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.},
archivePrefix = {arXiv},
arxivId = {1906.07682},
author = {Benedetti, Marcello and Lloyd, Erika and Sack, Stefan and Fiorentini, Mattia},
doi = {10.1088/2058-9565/ab4eb5},
eprint = {1906.07682},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benedetti et al. - 2019 - Parameterized quantum circuits as machine learning models.pdf:pdf},
issn = {20589565},
journal = {Quantum Science and Technology},
keywords = {Hybrid quantum-classical systems,Noisy intermediate-scale quantum technology,Quantum computing,Quantum machine learning},
number = {4},
pages = {1--18},
title = {{Parameterized quantum circuits as machine learning models}},
volume = {4},
year = {2019}
}
@article{Dunjko2018,
abstract = {Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research - quantum information versus machine learning (ML) and artificial intelligence (AI) - have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our 'big data' world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement - exploring what ML/AI can do for quantum physics and vice versa - researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.},
author = {Dunjko, Vedran and Briegel, Hans J.},
doi = {10.1088/1361-6633/aab406},
issn = {00344885},
journal = {Reports on Progress in Physics},
keywords = {Quantum computing,artifcial intelligence,machine learning,quantum information processing},
month = {jun},
number = {7},
pmid = {29504942},
publisher = {Institute of Physics Publishing},
title = {{Machine learning \& artificial intelligence in the quantum domain: A review of recent progress}},
volume = {81},
year = {2018}
}
@article{Grant2018,
abstract = {Quantum circuits with hierarchical structure have been used to perform binary classification of classical data encoded in a quantum state. We demonstrate that more expressive circuits in the same family achieve better accuracy and can be used to classify highly entangled quantum states, for which there is no known efficient classical method. We compare performance for several different parameterizations on two classical machine learning datasets, Iris and MNIST, and on a synthetic dataset of quantum states. Finally, we demonstrate that performance is robust to noise and deploy an Iris dataset classifier on the ibmqx4 quantum computer.},
archivePrefix = {arXiv},
arxivId = {1804.03680},
author = {Grant, Edward and Benedetti, Marcello and Cao, Shuxiang and Hallam, Andrew and Lockhart, Joshua and Stojevic, Vid and Green, Andrew G. and Severini, Simone},
doi = {10.1038/s41534-018-0116-9},
eprint = {1804.03680},
file = {:C\:/Users/Daniel/Downloads/66757dc2c7baa4d821ee82a0f51f8cb5.pdf:pdf},
issn = {20566387},
journal = {npj Quantum Information},
number = {1},
pages = {17--19},
title = {{Hierarchical quantum classifiers}},
volume = {4},
year = {2018}
}
@article{Havlicek2019a,
abstract = {Machine learning and quantum computing are two technologies that each have the potential to alter how computation is performed to address previously untenable problems. Kernel methods for machine learning are ubiquitous in pattern recognition, with support vector machines (SVMs) being the best known method for classification problems. However, there are limitations to the successful solution to such classification problems when the feature space becomes large, and the kernel functions become computationally expensive to estimate. A core element in the computational speed-ups enabled by quantum algorithms is the exploitation of an exponentially large quantum state space through controllable entanglement and interference. Here we propose and experimentally implement two quantum algorithms on a superconducting processor. A key component in both methods is the use of the quantum state space as feature space. The use of a quantum-enhanced feature space that is only efficiently accessible on a quantum computer provides a possible path to quantum advantage. The algorithms solve a problem of supervised learning: the construction of a classifier. One method, the quantum variational classifier, uses a variational quantum circuit1,2 to classify the data in a way similar to the method of conventional SVMs. The other method, a quantum kernel estimator, estimates the kernel function on the quantum computer and optimizes a classical SVM. The two methods provide tools for exploring the applications of noisy intermediate-scale quantum computers3 to machine learning.},
archivePrefix = {arXiv},
arxivId = {1804.11326},
author = {Havl{\'{i}}{\v{c}}ek, Vojt{\v{e}}ch and C{\'{o}}rcoles, Antonio D. and Temme, Kristan and Harrow, Aram W. and Kandala, Abhinav and Chow, Jerry M. and Gambetta, Jay M.},
doi = {10.1038/s41586-019-0980-2},
eprint = {1804.11326},
file = {:C\:/Users/Daniel/Downloads/10.1038@s41586-019-0980-2.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7747},
pages = {209--212},
pmid = {30867609},
publisher = {Springer US},
title = {{Supervised learning with quantum-enhanced feature spaces}},
url = {http://dx.doi.org/10.1038/s41586-019-0980-2},
volume = {567},
year = {2019}
}
@article{Havlicek2019,
abstract = {Machine learning and quantum computing are two technologies that each have the potential to alter how computation is performed to address previously untenable problems. Kernel methods for machine learning are ubiquitous in pattern recognition, with support vector machines (SVMs) being the best known method for classification problems. However, there are limitations to the successful solution to such classification problems when the feature space becomes large, and the kernel functions become computationally expensive to estimate. A core element in the computational speed-ups enabled by quantum algorithms is the exploitation of an exponentially large quantum state space through controllable entanglement and interference. Here we propose and experimentally implement two quantum algorithms on a superconducting processor. A key component in both methods is the use of the quantum state space as feature space. The use of a quantum-enhanced feature space that is only efficiently accessible on a quantum computer provides a possible path to quantum advantage. The algorithms solve a problem of supervised learning: the construction of a classifier. One method, the quantum variational classifier, uses a variational quantum circuit1,2 to classify the data in a way similar to the method of conventional SVMs. The other method, a quantum kernel estimator, estimates the kernel function on the quantum computer and optimizes a classical SVM. The two methods provide tools for exploring the applications of noisy intermediate-scale quantum computers3 to machine learning.},
archivePrefix = {arXiv},
arxivId = {1804.11326},
author = {Havl{\'{i}}{\v{c}}ek, Vojt{\v{e}}ch and C{\'{o}}rcoles, Antonio D. and Temme, Kristan and Harrow, Aram W. and Kandala, Abhinav and Chow, Jerry M. and Gambetta, Jay M.},
doi = {10.1038/s41586-019-0980-2},
eprint = {1804.11326},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Havl{\'{i}}{\v{c}}ek et al. - 2019 - Supervised learning with quantum-enhanced feature spaces.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7747},
pages = {209--212},
pmid = {30867609},
title = {{Supervised learning with quantum-enhanced feature spaces}},
volume = {567},
year = {2019}
}
@article{Kerenidis2020,
abstract = {Quantum machine learning and optimization are exciting new areas that have been brought forward by the breakthrough quantum algorithm of Harrow, Hassidim, and Lloyd for solving systems of linear equations. The utility of classical linear system solvers extends beyond linear algebra as they can be leveraged to solve optimization problems using iterative methods like gradient descent. In this work, we provide a quantum method for performing gradient descent when the gradient is an affine function. Performing $\tau$ steps of the gradient descent requires time O($\tau$CS) for weighted least-squares problems, where CS is the cost of performing one step of the gradient descent quantumly, which at times can be considerably smaller than the classical cost. We illustrate our method by providing two applications: first, for solving positive semidefinite linear systems, and, second, for performing stochastic gradient descent for the weighted least-squares problem with reduced quantum memory requirements. We also provide a quantum linear system solver in the QRAM data structure model that provides significant savings in cost for large families of matrices.},
archivePrefix = {arXiv},
arxivId = {1704.04992},
author = {Kerenidis, Iordanis and Prakash, Anupam},
doi = {10.1103/PhysRevA.101.022316},
eprint = {1704.04992},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kerenidis, Prakash - 2020 - Quantum gradient descent for linear systems and least squares.pdf:pdf},
issn = {24699934},
journal = {Physical Review A},
number = {2},
title = {{Quantum gradient descent for linear systems and least squares}},
volume = {101},
year = {2020}
}
@article{Lloyd2013,
abstract = {Machine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.},
archivePrefix = {arXiv},
arxivId = {1307.0411},
author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
eprint = {1307.0411},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lloyd, Mohseni, Rebentrost - 2013 - Quantum algorithms for supervised and unsupervised machine learning.pdf:pdf},
pages = {1--11},
title = {{Quantum algorithms for supervised and unsupervised machine learning}},
url = {http://arxiv.org/abs/1307.0411},
year = {2013}
}
@article{Otterbach2017,
abstract = {Machine learning techniques have led to broad adoption of a statistical model of computing. The statistical distributions natively available on quantum processors are a superset of those available classically. Harnessing this attribute has the potential to accelerate or otherwise improve machine learning relative to purely classical performance. A key challenge toward that goal is learning to hybridize classical computing resources and traditional learning techniques with the emerging capabilities of general purpose quantum processors. Here, we demonstrate such hybridization by training a 19-qubit gate model processor to solve a clustering problem, a foundational challenge in unsupervised learning. We use the quantum approximate optimization algorithm in conjunction with a gradient-free Bayesian optimization to train the quantum machine. This quantum/classical hybrid algorithm shows robustness to realistic noise, and we find evidence that classical optimization can be used to train around both coherent and incoherent imperfections.},
archivePrefix = {arXiv},
arxivId = {1712.05771},
author = {Otterbach, J. S. and Manenti, R. and Alidoust, N. and Bestwick, A. and Block, M. and Bloom, B. and Caldwell, S. and Didier, N. and {Schuyler Fried}, E. and Hong, S. and Karalekas, P. and Osborn, C. B. and Papageorge, A. and Peterson, E. C. and Prawiroatmodjo, G. and Rubin, N. and Ryan, Colm A. and Scarabelli, D. and Scheer, M. and Sete, E. A. and Sivarajah, P. and Smith, Robert S. and Staley, A. and Tezak, N. and Zeng, W. J. and Hudson, A. and Johnson, Blake R. and Reagor, M. and {Da Silva}, M. P. and Rigetti, C.},
eprint = {1712.05771},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Otterbach et al. - 2017 - Unsupervised machine learning on a hybrid quantum computer.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Unsupervised machine learning on a hybrid quantum computer}},
year = {2017}
}
@article{Rebentrost2014,
abstract = {Supervised machine learning is the classification of new data based on already classified training examples. In this work, we show that the support vector machine, an optimized binary classifier, can be implemented on a quantum computer, with complexity logarithmic in the size of the vectors and the number of training examples. In cases where classical sampling algorithms require polynomial time, an exponential speedup is obtained. At the core of this quantum big data algorithm is a nonsparse matrix exponentiation technique for efficiently performing a matrix inversion of the training data inner-product (kernel) matrix.},
archivePrefix = {arXiv},
arxivId = {1307.0471},
author = {Rebentrost, Patrick and Mohseni, Masoud and Lloyd, Seth},
doi = {10.1103/PhysRevLett.113.130503},
eprint = {1307.0471},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rebentrost, Mohseni, Lloyd - 2014 - Quantum support vector machine for big data classification.pdf:pdf},
issn = {10797114},
journal = {Physical Review Letters},
number = {3},
pages = {1--5},
title = {{Quantum support vector machine for big data classification}},
volume = {113},
year = {2014}
}
@article{Schuld2017,
abstract = {Lately, much attention has been given to quantum algorithms that solve pattern recognition tasks in machine learning. Many of these quantum machine learning algorithms try to implement classical models on large-scale universal quantum computers that have access to non-trivial subroutines such as Hamiltonian simulation, amplitude amplification and phase estimation. We approach the problem from the opposite direction and analyse a distance-based classifier that is realised by a simple quantum interference circuit. After state preparation, the circuit only consists of a Hadamard gate as well as two single-qubit measurements, and computes the distance between data points in quantum parallel. We demonstrate the proof-of-principle using the IBM Quantum Experience and analyse the performance of the classifier with numerical simulations, showing that it classifies surprisingly well for simple benchmark tasks.},
archivePrefix = {arXiv},
arxivId = {1703.10793},
author = {Schuld, M. and Fingerhuth, M. and Petruccione, F.},
doi = {10.1209/0295-5075/119/60002},
eprint = {1703.10793},
file = {:C\:/Users/Daniel/Downloads/schuld2017.pdf:pdf},
issn = {0295-5075},
journal = {EPL (Europhysics Letters)},
number = {6},
pages = {60002},
title = {{Implementing a distance-based classifier with a quantum interference circuit}},
volume = {119},
year = {2017}
}
@article{Tacchino2019,
abstract = {Artificial neural networks are the heart of machine learning algorithms and artificial intelligence. Historically, the simplest implementation of an artificial neuron traces back to the classical Rosenblatt's “perceptron”, but its long term practical applications may be hindered by the fast scaling up of computational complexity, especially relevant for the training of multilayered perceptron networks. Here we introduce a quantum information-based algorithm implementing the quantum computer version of a binary-valued perceptron, which shows exponential advantage in storage resources over alternative realizations. We experimentally test a few qubits version of this model on an actual small-scale quantum processor, which gives answers consistent with the expected results. We show that this quantum model of a perceptron can be trained in a hybrid quantum-classical scheme employing a modified version of the perceptron update rule and used as an elementary nonlinear classifier of simple patterns, as a first step towards practical quantum neural networks efficiently implemented on near-term quantum processing hardware.},
archivePrefix = {arXiv},
arxivId = {1811.02266},
author = {Tacchino, Francesco and Macchiavello, Chiara and Gerace, Dario and Bajoni, Daniele},
doi = {10.1038/s41534-019-0140-4},
eprint = {1811.02266},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tacchino et al. - 2019 - An artificial neuron implemented on an actual quantum processor.pdf:pdf},
issn = {20566387},
journal = {npj Quantum Information},
number = {1},
pages = {1--8},
publisher = {Springer US},
title = {{An artificial neuron implemented on an actual quantum processor}},
url = {http://dx.doi.org/10.1038/s41534-019-0140-4},
volume = {5},
year = {2019}
}
@article{Tang2018,
abstract = {We describe classical analogues to quantum algorithms for principal component analysis and nearest-centroid clustering. Given sampling assumptions, our classical algorithms run in time polylogarithmic in input, matching the runtime of the quantum algorithms with only polynomial slowdown. These algorithms are evidence that their corresponding problems do not yield exponential quantum speedups. To build our classical algorithms, we use the same techniques as applied in our previous work dequantizing a quantum recommendation systems algorithm. Thus, we provide further evidence for the strength of classical ℓ2-norm sampling assumptions when replacing quantum state preparation assumptions, in the machine learning domain.},
archivePrefix = {arXiv},
arxivId = {1811.00414},
author = {Tang, Ewin},
eprint = {1811.00414},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang - 2018 - Quantum-inspired classical algorithms for principal component analysis and supervised clustering.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--8},
title = {{Quantum-inspired classical algorithms for principal component analysis and supervised clustering}},
year = {2018}
}
@article{Tang2019,
abstract = {We give a classical analogue to Kerenidis and Prakash's quantum recommendation system, previously believed to be one of the strongest candidates for provably exponential speedups in quantum machine learning. Our main result is an algorithm that, given an m × n matrix in a data structure supporting certain ℓ2-norm sampling operations, outputs an ℓ2-norm sample from a rank-k approximation of that matrix in time O(poly(k) log(mn)), only polynomially slower than the quantum algorithm. As a consequence, Kerenidis and Prakash's algorithm does not in fact give an exponential speedup over classical algorithms. Further, under strong input assumptions, the classical recommendation system resulting from our algorithm produces recommendations exponentially faster than previous classical systems, which run in time linear in m and n. The main insight of this work is the use of simple routines to manipulate ℓ2-norm sampling distributions, which play the role of quantum superpositions in the classical setting. This correspondence indicates a potentially fruitful framework for formally comparing quantum machine learning algorithms to classical machine learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1807.04271},
author = {Tang, Ewin},
doi = {10.1145/3313276.3316310},
eprint = {1807.04271},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang - 2019 - A quantum-inspired classical algorithm for recommendation systems.pdf:pdf},
isbn = {9781450367059},
issn = {07378017},
journal = {Proceedings of the Annual ACM Symposium on Theory of Computing},
keywords = {Exponential speedup,Low-rank approximation,Quantum machine learning,Recommender systems,Sampling},
pages = {217--228},
title = {{A quantum-inspired classical algorithm for recommendation systems}},
year = {2019}
}
@article{Twomey1995,
abstract = {Model building in artificial neural networks (ANN) refers to selecting the "optimal" network architecture, network topology, data representation, training algorithm, training parameters, and terminating criteria, such that some desired level of performance is achieved. Validation, a critical aspect of any model construction, is based upon some specified ANN performance measure of data that was not used in model construction. In addition to trained ANN validation, this performance measure is often used to evaluate the superiority of network architecture, learning algorithm, or application of a neural network. This paper investigates the three most frequently reported performance measures for pattern classification networks: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and percent good classification. First the inconsistency of the three metrics for selecting the "better" network is examined empirically. An analysis of error histograms is shown to be an effective means for investigating and resolving inconsistent network performance measures. Second, the focus of this paper is on percent good classification, the most often used measure of performance for classification networks. This measure is satisfactory if no particular importance is given to any single class, however, if one class is deemed more serious than others, percent good classification will mask the individual class components. This deficiency is resolved through a neural network analogy to the statistical concept of power. It is shown that power as a neural network performance metric is tuneable, and is a more descriptive measure than percent correct for evaluating and predicting the "goodness" of a network. {\textcopyright} 1995.},
author = {Twomey, J.M. and Smith, A.E.},
doi = {10.1016/0895-7177(94)00207-5},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Twomey, Smith - 1995 - Performance measures, consistency, and power for artificial neural network models.pdf:pdf},
issn = {08957177},
journal = {Mathematical and Computer Modelling},
month = {jan},
number = {1-2},
pages = {243--258},
title = {{Performance measures, consistency, and power for artificial neural network models}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0895717794002075},
volume = {21},
year = {1995}
}
@article{Zhao2018,
abstract = {Bayesian methods in machine learning, such as Gaussian processes, have great advantages compared to other techniques. In particular, they provide estimates of the uncertainty associated with a prediction. Extending the Bayesian approach to deep architectures has remained a major challenge. Recent results connected deep feedforward neural networks with Gaussian processes, allowing training without backpropagation. This connection enables us to leverage a quantum algorithm designed for Gaussian processes and develop a new algorithm for Bayesian deep learning on quantum computers. The properties of the kernel matrix in the Gaussian process ensure the efficient execution of the core component of the protocol, quantum matrix inversion, providing an at least polynomial speedup over classical algorithms. Furthermore, we demonstrate the execution of the algorithm on contemporary quantum computers and analyze its robustness with respect to realistic noise models.},
archivePrefix = {arXiv},
arxivId = {1806.11463},
author = {Zhao, Zhikuan and Pozas-Kerstjens, Alejandro and Rebentrost, Patrick and Wittek, Peter},
doi = {10.1007/s42484-019-00004-7},
eprint = {1806.11463},
file = {:C\:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2018 - Bayesian deep learning on a quantum computer.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--11},
title = {{Bayesian deep learning on a quantum computer}},
year = {2018}
}
